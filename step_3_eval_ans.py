import argparse
from utils.eval_utils import eval_ans

if __name__ == "__main__":
    """
    Evaluates answers generated by a model and saves the evaluation results.
    """
    parser = argparse.ArgumentParser()

    parser.add_argument("--model_name", type=str, default="gpt-4o")

    parser.add_argument("--api_key", type=str)

    parser.add_argument("--data_dir", type=str,
                        help="The parent dir of dataset ")

    parser.add_argument("--save_dir", type=str, default="./result",
                        help="The local dir to save the evaluation result")

    args = parser.parse_args()

    eval_ans(args.model_name, args.api_key, args.data_dir, args.save_dir)
