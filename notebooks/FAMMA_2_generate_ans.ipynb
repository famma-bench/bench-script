{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import omegaconf\n",
    "from easyllm_kit.utils.io_utils import write_to_database\n",
    "from easyllm_kit.utils import get_logger, read_json\n",
    "from easyllm_kit.models import LLM\n",
    "from easyllm_kit.configs.llm_base_config import GenerationArguments\n",
    "# from famma_runner.runners.base_runner import Runner\n",
    "from famma_runner.utils import (\n",
    "    collect_images_from_first_subquestion, \n",
    "    generate_response_from_llm, \n",
    "    safe_parse_response,\n",
    "    QuestionPrompt, \n",
    "    LANGUAGE_ORDER, \n",
    "    DC, \n",
    "    order_by_language, \n",
    "    ProgramOfThoughtsQuestionPrompt\n",
    ")\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger('generation_runner', 'generation_runner.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = Path.joinpath(project_root, 'configs', 'gen_config_qwen.yaml')\n",
    "config = OmegaConf.load(config_dir)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def load_dataset(data_path):\n",
    "    \"\"\"Load dataset from JSON file and prepare it for processing.\"\"\"\n",
    "    data = read_json(data_path)\n",
    "    dataset_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create a new column for sorting languages\n",
    "    order_by_language(dataset_df, LANGUAGE_ORDER, DC.MAIN_QUESTION_ID, DC.SUB_QUESTION_ID, DC.LANGUAGE)\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset by question IDs\n",
    "def filter_dataset_by_question_id(dataset_df, question_ids):\n",
    "    \"\"\"Filter dataset by specific question_ids.\"\"\"\n",
    "    filtered_main_question_ids = None\n",
    "    \n",
    "    if not question_ids:\n",
    "        return dataset_df, filtered_main_question_ids\n",
    "    \n",
    "    # Convert single question_id to list for consistent processing\n",
    "    if not isinstance(question_ids, omegaconf.ListConfig) and not isinstance(question_ids, list):\n",
    "        question_ids = [question_ids]\n",
    "    \n",
    "    if len(question_ids) == 0:\n",
    "        return dataset_df, filtered_main_question_ids\n",
    "    \n",
    "    # Create empty DataFrame to collect all filtered results\n",
    "    filtered_results = pd.DataFrame()\n",
    "    \n",
    "    # Track unique language-main_question_id pairs to avoid duplicate processing\n",
    "    processed_pairs = set()\n",
    "    \n",
    "    for question_id in question_ids:\n",
    "        try:\n",
    "            # Parse the question_id to extract components\n",
    "            parts = question_id.split('_')\n",
    "            if len(parts) >= 3:  # Ensure we have at least language, main_question_id, and sub_question_id\n",
    "                language = parts[0]\n",
    "                main_question_id = int(parts[1])\n",
    "                \n",
    "                # Create a unique identifier for this language-main_question_id pair\n",
    "                pair_key = f\"{language}_{main_question_id}\"\n",
    "                \n",
    "                # Skip if we've already processed this pair\n",
    "                if pair_key in processed_pairs:\n",
    "                    continue\n",
    "                \n",
    "                processed_pairs.add(pair_key)\n",
    "                \n",
    "                logger.info(f\"Filtering dataset for language: {language}, main_question_id: {main_question_id}\")\n",
    "                \n",
    "                # Filter the dataset by both main_question_id and language\n",
    "                current_filtered = dataset_df[\n",
    "                    (dataset_df[DC.MAIN_QUESTION_ID] == main_question_id) & \n",
    "                    (dataset_df[DC.LANGUAGE] == language)\n",
    "                ]\n",
    "                \n",
    "                if current_filtered.empty:\n",
    "                    logger.warning(f\"No matching questions found for {question_id}\")\n",
    "                else:\n",
    "                    logger.info(f\"Found {len(current_filtered)} questions matching {question_id}\")\n",
    "                    # Append to our results\n",
    "                    filtered_results = pd.concat([filtered_results, current_filtered])\n",
    "            else:\n",
    "                logger.warning(f\"Invalid question_id format: {question_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing question_id {question_id}: {str(e)}\")\n",
    "    \n",
    "    # If we didn't find any matches, return the original dataset\n",
    "    if filtered_results.empty:\n",
    "        logger.warning(\"No matching questions found for any of the provided question_ids\")\n",
    "        return dataset_df, None\n",
    "    logger.info(f\"Total of {len(filtered_results)} questions matched across all filters\")\n",
    "    \n",
    "    # Extract unique main_question_ids from filtered results\n",
    "    filtered_main_question_ids = filtered_results[DC.MAIN_QUESTION_ID].unique().tolist()\n",
    "    return filtered_results, filtered_main_question_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = load_dataset(config['data']['data_dir'])\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data']['question_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df, filtered_main_question_ids = filter_dataset_by_question_id(\n",
    "        dataset_df, config['data']['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_df) ,filtered_main_question_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setup the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the language model\n",
    "def setup_model(model_config, generation_args):\n",
    "    \"\"\"Initialize and setup the language model.\"\"\"\n",
    "    # Convert generation_args to GenerationArguments if needed\n",
    "    if isinstance(generation_args, dict):\n",
    "        generation_config = GenerationArguments(**generation_args)\n",
    "    else:\n",
    "        generation_config = generation_args\n",
    "        \n",
    "    # Build the LLM model\n",
    "    llm_config = {\n",
    "        'model_config': model_config,\n",
    "        'generation_config': generation_config\n",
    "    }\n",
    "\n",
    "    # If using custom model, load it from custom_llm.py\n",
    "    if model_config.get(\"model_name\") == \"custom_llm\":\n",
    "        from custom_llm import MyCustomModel\n",
    "        \n",
    "    llm = LLM.build_from_config(llm_config)\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setup OCR if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OCR if needed\n",
    "def setup_ocr(use_ocr=False):\n",
    "    \"\"\"Setup OCR model if required.\"\"\"\n",
    "    ocr_model = None\n",
    "    if use_ocr:\n",
    "        try:\n",
    "            from paddleocr import PaddleOCR\n",
    "            ocr_model = PaddleOCR(use_angle_cls=True)\n",
    "            logger.info(\"OCR model initialized successfully\")\n",
    "        except ImportError:\n",
    "            logger.error(\"Failed to import PaddleOCR. Please install with: pip install paddleocr\")\n",
    "    return ocr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = config.get(\"generation\", {})\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config[\"model\"]\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = setup_model(model_config, generation_config)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = model_config.get(\"model_full_name\", \"unknown_model\")\n",
    "llm_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ocr = model_config.get('use_ocr', False)\n",
    "ocr_model = setup_ocr(use_ocr)\n",
    "ocr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dictdatabase as DDB\n",
    "DDB.config.storage_directory = Path.joinpath(project_root, 'ddb_storage')\n",
    "def initialize_database(output_db: str):\n",
    "    \"\"\"\n",
    "    Initialize the database if it doesn't exist and return the database object.\n",
    "    \"\"\"\n",
    "    db = DDB.at(output_db).read()\n",
    "    if db is None:\n",
    "        DDB.at(output_db).create()\n",
    "        db = {}\n",
    "        logger.info(f\"Initialized new database: {output_db}\")\n",
    "    else:\n",
    "        logger.info(f\"Loaded existing database: {output_db} with {len(db)} entries.\")\n",
    "    return db\n",
    "\n",
    "def setup_database(model_name, data_dir):\n",
    "    \"\"\"Initialize database for storing results.\"\"\"\n",
    "    release_version = data_dir.split('/')[-1].split('.')[0]\n",
    "    target_db_name = f'{model_name}_ans_{release_version}'\n",
    "    target_db = initialize_database(output_db=target_db_name)\n",
    "    \n",
    "    return target_db_name, target_db\n",
    "\n",
    "def save_results(target_db_name, key, subquestion_responses):\n",
    "    \"\"\"Save responses to database.\"\"\"\n",
    "    write_to_database(target_db_name, key, subquestion_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data']['data_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_db_name, target_db = setup_database(llm_name, config['data']['data_dir'])\n",
    "target_db_name, target_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Question Processing and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_for_one_main_question(sub_question_set_df, llm, use_pot=False, \n",
    "                                          is_reasoning_model=False, use_ocr=False, \n",
    "                                          ocr_model=None, parent_dir=\"\"):\n",
    "    \"\"\"Generate model responses for a set of related sub-questions.\"\"\"\n",
    "    # Get the context from the first sub_question\n",
    "    context = sub_question_set_df.iloc[0].get(\"context\", \"\")\n",
    "\n",
    "    # Collect images from the first sub-question\n",
    "    images = collect_images_from_first_subquestion(sub_question_set_df, parent_dir=parent_dir)\n",
    "\n",
    "    # Format sub-questions\n",
    "    sub_questions = []\n",
    "    question_id_list = []\n",
    "    for _, row in sub_question_set_df.iterrows():\n",
    "        question_dict = {\n",
    "            \"id\": row['question_id'],\n",
    "            \"type\": row['question_type'],\n",
    "            \"question\": row['question']\n",
    "        }\n",
    "\n",
    "        # Add options if it's a multiple-choice question\n",
    "        if row['question_type'] == 'multiple-choice':\n",
    "            question_dict[\"options\"] = row['options']\n",
    "\n",
    "        sub_questions.append(question_dict)\n",
    "        question_id_list.append(row['question_id'])\n",
    "\n",
    "    # Select prompt based on configuration\n",
    "    if use_pot:\n",
    "        prompt = ProgramOfThoughtsQuestionPrompt.init().format(\n",
    "            context=context,\n",
    "            sub_questions=sub_questions\n",
    "        )\n",
    "    else:\n",
    "        prompt = QuestionPrompt.init().format(\n",
    "            context=context,\n",
    "            sub_questions=sub_questions\n",
    "        )\n",
    "\n",
    "    # Generate response\n",
    "    model_output = generate_response_from_llm(llm, prompt, images, use_ocr=use_ocr, ocr_model=ocr_model)\n",
    "    model_response = safe_parse_response(model_output, question_id_list, is_reasoning_model=is_reasoning_model)\n",
    "\n",
    "    return model_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Main Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get additional configuration parameters\n",
    "use_pot = model_config.get('use_pot', False)\n",
    "is_reasoning_model = model_config.get('is_reasoning_model', False)\n",
    "# Create a copy of the DataFrame at the start\n",
    "df_copy = dataset_df.copy()\n",
    "    # Get parent directory for image paths\n",
    "parent_dir = os.path.dirname(config['data']['data_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each main question group\n",
    "for (_, language, main_question_id), group in df_copy.groupby(\n",
    "        ['language_order', DC.LANGUAGE, DC.MAIN_QUESTION_ID]):\n",
    "    \n",
    "    key = f'{language}_{main_question_id}'\n",
    "    \n",
    "    # Skip if already in database AND not specifically requested in filtered_main_question_ids\n",
    "    if key in target_db and (filtered_main_question_ids is None or \n",
    "                                main_question_id not in filtered_main_question_ids):\n",
    "        logger.info(f'Skipping {key} - already in database and not specifically requested')\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        logger.info(f'Start generating answers for {language} -- main_question_id: {main_question_id}')\n",
    "        \n",
    "        # Generate answers for this question group\n",
    "        model_response = generate_answer_for_one_main_question(\n",
    "            group, llm, use_pot=use_pot, is_reasoning_model=is_reasoning_model,\n",
    "            use_ocr=use_ocr, ocr_model=ocr_model, parent_dir=parent_dir\n",
    "        )\n",
    "        \n",
    "        # Aggregate all subquestions with their answers \n",
    "        subquestion_responses = {}\n",
    "        for idx in range(len(group)):\n",
    "            output_key = group.iloc[idx]['question_id']\n",
    "            \n",
    "            # Create a JSON object with the original input data and the model response\n",
    "            input_data_with_response = group.iloc[idx].to_dict()\n",
    "            \n",
    "            if is_reasoning_model:\n",
    "                input_data_with_response.update({\n",
    "                    'model_answer': model_response[output_key].get('answer', ''),\n",
    "                    'model_explanation': model_response[output_key].get('explanation', ''),\n",
    "                    'model_reasoning': model_response.get('reasoning_content', '')\n",
    "                })\n",
    "            else:\n",
    "                input_data_with_response.update({\n",
    "                    'model_answer': model_response[output_key].get('answer', ''),\n",
    "                    'model_explanation': model_response[output_key].get('explanation', '')\n",
    "                })\n",
    "            \n",
    "            # Store the response in the subquestion_responses dictionary\n",
    "            subquestion_responses[output_key] = input_data_with_response\n",
    "        \n",
    "        # Write the aggregated subquestion responses to the database\n",
    "        save_results(target_db_name, key, subquestion_responses)\n",
    "        logger.info(f'Successfully processed and saved {key}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing main_question_id {main_question_id}: {str(e)}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "famma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
