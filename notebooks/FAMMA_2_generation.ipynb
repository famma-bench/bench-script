{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/famma-bench/bench-script/blob/main/notebooks/FAMMA_3_generation.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook explains how to generate answers for the **FAMMA** benchmark.  \n",
    "Before you start, complete the following prerequisites:\n",
    "\n",
    "1. **Install the FAMMA benchmark scripts**  \n",
    "2. **Set up access to a language model**—either through a web API or a locally hosted LLM  \n",
    "\n",
    "---\n",
    "\n",
    "## Install the benchmark scripts\n",
    "\n",
    "Run the commands below to download and install the FAMMA utilities:\n"
   ],
   "metadata": {
    "id": "dCH5Nu2lJ9Ht"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! rm -rf bench-script\n",
    "! git clone https://github.com/famma-bench/bench-script.git\n",
    "! pip install -r bench-script/requirements.txt\n",
    "! pip install -U datasets\n",
    "# Install the package in editable mode using the notebook's pip\n",
    "# ! pip install -e ./bench-script/"
   ],
   "metadata": {
    "id": "Sa8QagNJJ9uh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can download the dataset by running the following script."
   ],
   "metadata": {
    "id": "yzVOgxFyKNdZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./bench-script\")\n",
    "\n",
    "from famma_runner.utils.data_utils import download_data\n",
    "\n",
    "# the directory of the dataset in huggingface\n",
    "hf_dir = \"weaverbirdllm/famma\"\n",
    "\n",
    "# the version of the dataset, there are two versions: release_basic and release_livepro\n",
    "# if None, it will download the whole dataset\n",
    "split = \"release_livepro\"\n",
    "\n",
    "# the local directory to save the dataset\n",
    "save_dir = \"./data\"\n",
    "\n",
    "# whether to download the dataset from huggingface or local, by default it is False\n",
    "from_local = False\n",
    "\n",
    "success = download_data(\n",
    "        hf_dir=hf_dir,\n",
    "        split=split,\n",
    "        save_dir=save_dir,\n",
    "        from_local=from_local\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nq0YrgqKA-b",
    "outputId": "74a42b7b-a1ff-4d21-a7bc-04cb0663b408"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved release_livepro split to ./data/release_livepro.json\n",
      "\n",
      "Dataset downloaded and saved to ./data\n",
      "Images are saved in ./data/images_release_livepro\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "After downloading, the dataset will be saved in the local directory `./data` in json format.  \n",
    "\n",
    "# Answer Generation\n",
    "\n",
    "## Calling a Custom LLM\n",
    "\n",
    "In most cases we can simply plug in any model supported by `easyllm_kit`, but if we wish to invoke a proprietary endpoint-- e.g., a Deepseek-R1 instance deployed on Alibaba Cloud-- we only need to wrap that endpoint in a thin adapter that subclasses `easyllm_kit.models.base.LLM`. The snippet below shows a minimal implementation that registers a custom client under the handle `custom_llm` and forwards prompts to the remote API."
   ],
   "metadata": {
    "id": "22TAd7_I11l-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from easyllm_kit.models.base import LLM\n",
    "\n",
    "\n",
    "# Define your custom model class\n",
    "@LLM.register(\"custom_llm\")\n",
    "class MyReasoningModel(LLM):\n",
    "    model_name = 'custom_llm'\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # Ensure the base class is initialized correctly\n",
    "        # Initialize your model here\n",
    "        import openai\n",
    "        self.model_config = config['model_config']\n",
    "        self.generation_config = config['generation_config']\n",
    "        self.client = openai.OpenAI(api_key=self.model_config.api_key,\n",
    "                                    base_url=self.model_config.api_url,\n",
    "                                    timeout=1800)\n",
    "\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_config.model_full_name,\n",
    "            max_tokens=self.generation_config.max_length,\n",
    "            temperature=self.generation_config.temperature,\n",
    "            top_p=self.generation_config.top_p,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        reasoning_content = \"\"\n",
    "        if hasattr(response.choices[0].message, 'reasoning_content'):\n",
    "            reasoning_content = response.choices[0].message.reasoning_content\n",
    "        content = response.choices[0].message.content\n",
    "        return  {'content': content, 'reasoning_content': reasoning_content}\n",
    "\n"
   ],
   "metadata": {
    "id": "kh-WW-iW4N4I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an illustrative example, we write the YAML content to a file yaml_content as below. Note that runner_config is the pipeline configuration ID that tells `famma_runner` which configuration to use. We combine the data configuration and model configuration into a single YAML file. This file will be used to initialize the model runner, which will handle the training process based on the specified parameters.\n",
    "\n",
    "For simplicity, we run over only one question -- `english_1_1_r2` (runner will answer with its all subquestions) and use `qwen-vl-max` as the model."
   ],
   "metadata": {
    "id": "1kfS7X6e--0Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "yaml_content = \"\"\"\n",
    "runner_name: generation\n",
    "\n",
    "data:\n",
    "  data_dir: ./data/release_livepro.json\n",
    "  question_id: english_1_1_r4  # suppose we generate answerns only for question 1\n",
    "\n",
    "model:\n",
    "  model_name: custom_llm # register name of your custom model\n",
    "  api_key: sk-xxxx  # put your api key here\n",
    "  api_url: https://dashscope.aliyuncs.com/compatible-mode/v1\n",
    "  model_full_name: qwen-vl-max # put the model name here\n",
    "  use_ocr: false\n",
    "  use_pot: false\n",
    "  is_reasoning_model: false\n",
    "\n",
    "generation:\n",
    "  temperature: 0.0\n",
    "  top_p: 0.9\n",
    "  max_length: 1024\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Save the content to a file named config.yaml\n",
    "with open(\"config.yaml\", \"w\") as file:\n",
    "    file.write(yaml_content)"
   ],
   "metadata": {
    "id": "3n07F5pv8Zbm"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can run the snippet below to verify that the configuration is working correctly."
   ],
   "metadata": {
    "id": "GqRgBiSlS2QI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config_dir = \"config.yaml\"\n",
    "config = OmegaConf.load(config_dir)\n",
    "\n",
    "\n",
    "# Build the LLM model\n",
    "llm_config = {'model_config': config.get('model', None),\n",
    "              'generation_config': config.get('generation', None), }\n",
    "custom_model = LLM.build_from_config(llm_config)\n",
    "output = custom_model.generate(\"What is the impact of rising tarrif on china\")\n",
    "print(output)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEYv-S6W5OQq",
    "outputId": "3ba2fa1f-6d71-44c4-a413-d742d4923dd7"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'content': \"The impact of rising tariffs on China can be multifaceted, affecting various aspects of its economy and global trade relationships. Here are some key impacts:\\n\\n1. **Economic Growth**: Rising tariffs can slow down China's economic growth by reducing exports, which is a significant driver of the Chinese economy. Higher costs for Chinese goods in foreign markets can lead to decreased demand.\\n\\n2. **Trade Balance**: Tariffs can disrupt China's trade balance. As tariffs increase, the cost of Chinese exports rises, potentially leading to a decrease in exports and an increase in imports, thus worsening the trade deficit.\\n\\n3. **Inflation**: Higher tariffs can lead to increased prices for imported goods in China, contributing to inflationary pressures. This can affect both consumers and businesses that rely on imported materials and components.\\n\\n4. **Business Costs**: For Chinese companies, higher tariffs mean increased production costs if they rely on imported raw materials or components. This can reduce profit margins and competitiveness.\\n\\n5. **Employment**: If Chinese industries suffer due to reduced export demand, there could be job losses, particularly in sectors heavily reliant on exports.\\n\\n6. **Currency Value**: Tariffs can influence the value of the Chinese yuan (RMB). If exports decline significantly, the yuan might depreciate, making Chinese goods cheaper for foreign buyers but also increasing the cost of imports.\\n\\n7. **Supply Chain Disruptions**: Companies may look to diversify their supply chains away from China to avoid tariffs, which can lead to long-term shifts in global manufacturing and sourcing patterns.\\n\\n8. **Investment Climate**: The uncertainty created by rising tariffs can deter foreign investment in China, as businesses may be hesitant to commit resources in an unstable trade environment.\\n\\n9. **Domestic Consumption**: To counterbalance the negative effects on exports, China may focus more on stimulating domestic consumption and developing its internal market.\\n\\n10. **Geopolitical Relations**: Rising tariffs can strain diplomatic relations between China and other countries, particularly those imposing the tariffs, such as the United States. This can have broader implications for international cooperation and geopolitical stability.\\n\\nOverall, while China has shown resilience in adapting to external pressures, rising tariffs pose significant challenges that require strategic responses to mitigate adverse effects.\", 'reasoning_content': ''}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finnaly, we use the following script to run the overall generation for FAMMA."
   ],
   "metadata": {
    "id": "kopwgbobTkdP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "from famma_runner.runners import Runner\n",
    "\n",
    "\"\"\"\n",
    "Generate answers from a specified model and save the results to files.\n",
    "\"\"\"\n",
    "\n",
    "config = OmegaConf.load('config.yaml')\n",
    "\n",
    "runner = Runner.build_from_config(config)\n",
    "\n",
    "runner.run()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HKTU7VYbGVrV",
    "outputId": "d39ef5b3-0f76-40ac-a2c9-b6853ae8bfe5"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32m2025-05-18 12:21:31, generation_runner [generation_runner.filter_dataset_by_question_id:94] INFO - Filtering dataset for language: english, main_question_id: 1\u001B[39m\n",
      "\u001B[32m2025-05-18 12:21:31, generation_runner [generation_runner.filter_dataset_by_question_id:105] INFO - Found 5 questions matching english_1_1_r4\u001B[39m\n",
      "\u001B[32m2025-05-18 12:21:31, generation_runner [generation_runner.filter_dataset_by_question_id:118] INFO - Total of 5 questions matched across all filters\u001B[39m\n",
      "\u001B[32m2025-05-18 12:21:31, easyllm_kit [easyllm_kit.initialize_database:21] INFO - Initialized new database: qwen-vl-max_ans_release_livepro\u001B[39m\n",
      "\u001B[32m2025-05-18 12:21:31, generation_runner [generation_runner.run:204] INFO - start generating answers for english -- main_question_id: 1\u001B[39m\n",
      "\u001B[32m2025-05-18 12:22:00, easyllm_kit [easyllm_kit.write_to_database:35] INFO - Stored answer for record_idx english_1.\u001B[39m\n",
      "\u001B[32m2025-05-18 12:22:00, generation_runner [generation_runner.run:240] INFO - Generation complete\u001B[39m\n",
      "\u001B[32m2025-05-18 12:22:00, generation_runner [generation_runner.run:241] INFO - Result saved to qwen-vl-max_ans_release_livepro in json format\u001B[39m\n",
      "\u001B[32m2025-05-18 12:22:00, generation_runner [generation_runner.run:242] INFO - Result saved to output_samples.csv in csv format\u001B[39m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using pre-wrapped models in `easyllm_kit`\n",
    "\n",
    "`easyllm_kit` exposes several popular LLM endpoints through an OpenAI-style interface.  \n",
    "To invoke one, simply supply:\n",
    "\n",
    "- **`model_name`** – the provider key recognised by `easyllm_kit`\n",
    "- **`model_full_name`** – the exact model identifier offered by that provider\n",
    "\n",
    "| `model_name`        |  `model_full_name`  |\n",
    "|---------------------|----------------------------------|\n",
    "| `gpt4o`             | `o1`, `o1-mini`, `gpt-4o` |\n",
    "| `claude_35_sonnet`  | `claude-3-5-sonnet-20240620` (default) |\n",
    "| `gemini`            | Any Gemini model ID published by Google |\n",
    "\n",
    "\n",
    "To resue the above config, we simple modify the `model_name`, `model_full_name` along with `api_key` to call got-4o using `easyllm_kit`."
   ],
   "metadata": {
    "id": "Ir10EJ7JWbjK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "yaml_content = \"\"\"\n",
    "runner_name: generation\n",
    "\n",
    "data:\n",
    "  data_dir: ./data/release_livepro.json\n",
    "  question_id: english_1_1_r4  # suppose we generate answerns only for question 1\n",
    "\n",
    "model:\n",
    "  model_name: gpt4o\n",
    "  model_full_name: gpt-4o\n",
    "  use_api: true\n",
    "  api_key: sk-proj-xxx\n",
    "  use_litellm_api: false. # set to false to not use litellm api\n",
    "  use_ocr: false\n",
    "  use_pot: false\n",
    "  is_reasoning_model: false\n",
    "\n",
    "generation:\n",
    "  temperature: 0.0\n",
    "  top_p: 0.9\n",
    "  max_length: 1024\n",
    "\"\"\"\n",
    "\n",
    "# Save the content to a file named config.yaml\n",
    "with open(\"config_easyllm.yaml\", \"w\") as file:\n",
    "    file.write(yaml_content)\n"
   ],
   "metadata": {
    "id": "Mz3Arh8RXZ3h"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "config = OmegaConf.load(\"config_easyllm.yaml\")\n",
    "\n",
    "runner = Runner.build_from_config(config)\n",
    "\n",
    "runner.run()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNBu-Qoqdl2T",
    "outputId": "df847f7d-9429-440d-b354-9cf227d03263"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[32m2025-05-18 13:11:00, generation_runner [generation_runner.filter_dataset_by_question_id:94] INFO - Filtering dataset for language: english, main_question_id: 1\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:00, generation_runner [generation_runner.filter_dataset_by_question_id:105] INFO - Found 5 questions matching english_1_1_r4\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:00, generation_runner [generation_runner.filter_dataset_by_question_id:118] INFO - Total of 5 questions matched across all filters\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:00, easyllm_kit [easyllm_kit.initialize_database:21] INFO - Initialized new database: gpt-4o_ans_release_livepro\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:00, generation_runner [generation_runner.run:204] INFO - start generating answers for english -- main_question_id: 1\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:09, easyllm_kit [easyllm_kit.write_to_database:35] INFO - Stored answer for record_idx english_1.\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:09, generation_runner [generation_runner.run:240] INFO - Generation complete\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:09, generation_runner [generation_runner.run:241] INFO - Result saved to gpt-4o_ans_release_livepro in json format\u001B[39m\n",
      "\u001B[32m2025-05-18 13:11:09, generation_runner [generation_runner.run:242] INFO - Result saved to output_samples.csv in csv format\u001B[39m\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "AWyYbwb8fHMw"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
